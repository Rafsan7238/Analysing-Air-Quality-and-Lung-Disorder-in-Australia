{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install elasticsearch8 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.constants import *\n",
    "fission_url = 'http://127.0.0.1:9090'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"air_quality_hourly_avg\": \"Already Exists\", \"asthma_by_region\": \"Already Exists\", \"census_g21b\": \"Already Exists\", \"historic_tweet_sentiments\": \"Already Exists\", \"mortality_females\": \"Already Exists\", \"mortality_males\": \"Already Exists\", \"mortality_persons\": \"Already Exists\", \"rainfall_adelaide\": \"Already Exists\", \"rainfall_brisbane\": \"Already Exists\", \"rainfall_canberra\": \"Already Exists\", \"rainfall_darwin\": \"Already Exists\", \"rainfall_melbourne\": \"Already Exists\", \"rainfall_perth\": \"Already Exists\", \"rainfall_sydney\": \"Already Exists\", \"rainfall_tasmania\": \"Already Exists\", \"temperature_adelaide\": \"Already Exists\", \"temperature_brisbane\": \"Already Exists\", \"temperature_canberra\": \"Already Exists\", \"temperature_darwin\": \"Already Exists\", \"temperature_melbourne\": \"Already Exists\", \"temperature_perth\": \"Already Exists\", \"temperature_sydney\": \"Already Exists\", \"temperature_tasmania\": \"Already Exists\", \"mastodon_observations\": \"Already Exists\", \"bom_observations\": \"Already Exists\", \"stations\": \"Already Exists\"}'\n"
     ]
    }
   ],
   "source": [
    "result = requests.post(f'{fission_url}/indexes/create/all', {})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catch up old BOM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.post(fission_url + f'/elastic/{BOM_OBSERVATIONS}/documents').content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catch up old Mastodon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.post(fission_url + f'/elastic/{MASTODON}/documents').content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload Bom Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'stations already has data. Please delete before attempting re-insertion.'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_stations.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{STATIONS}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload hourly air quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n",
      "b''\n"
     ]
    }
   ],
   "source": [
    "with open('./2022_All_sites_air_quality_hourly_avg.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    chunk_size = 5000\n",
    "    curr_chunk = 1\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        if num > chunk_size * curr_chunk:\n",
    "            curr_chunk += 1\n",
    "            result = requests.post(fission_url + f'/elastic/{AIR_QUALITY_HOURLY_AVG}/documents', json=data)\n",
    "            print(result.content)\n",
    "            data = {}\n",
    "    result = requests.post(fission_url + f'/elastic/{AIR_QUALITY_HOURLY_AVG}/documents', json=data)\n",
    "    print(result.content)\n",
    "    # Returns errors. Hopefully you only see empty messages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload asthma by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(15, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/abs_2021census_g21a_aust_gccsa.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{ASTHMA_BY_REGION_INDEX_NAME}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload census g21b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(15, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/abs_2021census_g21b_aust_gccsa.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{CENSUS_G21B}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload historic tweet sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(340, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./historic_tweet_sentiments.json', 'r', encoding='utf-8-sig') as f:\n",
    "    data = json.load(f)\n",
    "    result = requests.post(fission_url + f'/elastic/{HIST_TWEET_INDEX_NAME}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload mortality female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(15, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/aihw_cimar_mortality_females_gccsa_2009.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{MORTALITY_FEMALES}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload mortality males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(15, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/aihw_cimar_mortality_males_gccsa_2009.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{MORTALITY_MALES}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload mortality persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(15, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/aihw_cimar_mortality_persons_gccsa_2009.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{MORTALITY_PERSONS}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall adelaide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(1765, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Adelaide.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_ADELAIDE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Brisbane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(286, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Brisbane.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_BRISBANE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Canberra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(596, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Canberra.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_CANBERRA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(793, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Darwin.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_DARWIN}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Melbourne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(636, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Melbourne.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_MELBOURNE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Perth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(373, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Perth.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_PERTH}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Sydney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(636, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Sydney.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_SYDNEY}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload rainfall Tasmania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(1707, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Tasmania.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_TASMANIA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Adelaide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(1188, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Adelaide.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_ADELAIDE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Brisbane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(292, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Brisbane.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_BRISBANE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Canberra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(187, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Canberra.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_CANBERRA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Darwin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(995, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Darwin.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_DARWIN}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Melbourne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(130, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Melbourne.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_MELBOURNE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Perth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(363, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Perth.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_PERTH}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Sydney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(78, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Sydney.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_SYDNEY}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload temperature Tasmania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(1700, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Tasmania.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_TASMANIA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
