{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import sys\n",
    "import json\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install elasticsearch8 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.constants import *\n",
    "from backend.elastic_client_provider import *\n",
    "fission_url = 'http://127.0.0.1:9090'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"air_quality_hourly_avg\": \"Already Exists\", \"asthma_by_region\": \"Already Exists\", \"census_g21b\": \"Already Exists\", \"historic_tweet_sentiments\": \"Already Exists\", \"mortality_females\": \"Already Exists\", \"mortality_males\": \"Already Exists\", \"mortality_persons\": \"Already Exists\", \"rainfall_adelaide\": \"Already Exists\", \"rainfall_brisbane\": \"Already Exists\", \"rainfall_canberra\": \"Already Exists\", \"rainfall_darwin\": \"Already Exists\", \"rainfall_melbourne\": \"Already Exists\", \"rainfall_perth\": \"Already Exists\", \"rainfall_sydney\": \"Already Exists\", \"rainfall_tasmania\": \"Already Exists\", \"temperature_adelaide\": \"Created\", \"temperature_brisbane\": \"Created\", \"temperature_canberra\": \"Created\", \"temperature_darwin\": \"Created\", \"temperature_melbourne\": \"Created\", \"temperature_perth\": \"Created\", \"temperature_sydney\": \"Created\", \"temperature_tasmania\": \"Created\"}'\n"
     ]
    }
   ],
   "source": [
    "result = requests.post(f'{fission_url}/indexes/create/all', {})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Upload hourly air quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'5002 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'\"Connection timed out\"'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'5000 documents added'\n",
      "b'200 documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./2022_All_sites_air_quality_hourly_avg.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    chunk_size = 5000\n",
    "    curr_chunk = 1\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        if num > chunk_size * curr_chunk:\n",
    "            curr_chunk += 1\n",
    "            result = requests.post(fission_url + f'/elastic/{AIR_QUALITY_HOURLY_AVG}/documents', json=data)\n",
    "            print(result.content)\n",
    "            data = {}\n",
    "    result = requests.post(fission_url + f'/elastic/{AIR_QUALITY_HOURLY_AVG}/documents', json=data)\n",
    "    print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/abs_2021census_g21a_aust_gccsa.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{ASTHMA_BY_REGION_INDEX_NAME}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/abs_2021census_g21b_aust_gccsa.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{CENSUS_G21B}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./historic_tweet_sentiments.json', 'r', encoding='utf-8-sig') as f:\n",
    "    data = json.load(f)\n",
    "    result = requests.post(fission_url + f'/elastic/{HIST_TWEET_INDEX_NAME}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/aihw_cimar_mortality_females_gccsa_2009.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{MORTALITY_FEMALES}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/aihw_cimar_mortality_males_gccsa_2009.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{MORTALITY_MALES}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./lung_disease_dataset/aihw_cimar_mortality_persons_gccsa_2009.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{MORTALITY_PERSONS}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Adelaide.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_ADELAIDE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Brisbane.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_BRISBANE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Canberra.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_CANBERRA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Darwin.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_DARWIN}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Melbourne.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_MELBOURNE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Perth.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_PERTH}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Sydney.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_SYDNEY}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'success documents added'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/rainfall_cities/Tasmania.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{RAINFALL_TASMANIA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'temperature_adelaide already has data. Please delete before attempting re-insertion.'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Adelaide.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_ADELAIDE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(292, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Brisbane.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_BRISBANE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(187, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Canberra.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_CANBERRA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(995, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Darwin.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_DARWIN}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(130, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Melbourne.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_MELBOURNE}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(363, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Perth.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_PERTH}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(78, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Sydney.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_SYDNEY}/documents', json=data)\n",
    "    print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'(1700, [])'\n"
     ]
    }
   ],
   "source": [
    "with open('./bom_historic_data/temperature_cities/Tasmania.csv', 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=',')\n",
    "    data = {}\n",
    "    for num, row in enumerate(csv_reader):\n",
    "        data[num] = row\n",
    "        \n",
    "    result = requests.post(fission_url + f'/elastic/{TEMPERATURE_TASMANIA}/documents', json=data)\n",
    "    print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
